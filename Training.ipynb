{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from Dataset import CreateDataset\n",
    "from Deeplabv3p import DeepLabv3p\n",
    "from Deeplabv3_mid import DeepLabv3p_mid, SimAM\n",
    "from GourmetNet import GourmetNet\n",
    "from Unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary {name_of_class: class_id} remember background has id 0\n",
    "DATASET_NAME = \"camerfood10\"\n",
    "\n",
    "if DATASET_NAME == \"camefood10\":\n",
    "   class_name_dict = {\"background\":0, \"taro\": 1, \"sauce jaune\": 2, \"koki\": 3, \"haricot rouge ou noir\": 4, \"water fufu\": 5, \"riz blanc\": 6, \n",
    "   \"baton de manioc\": 7, \"beignet farine de ble\": 8, \"sauce tomate\": 9, \"frites de plantains\": 10}\n",
    "\n",
    "elif \"brazillian\":\n",
    "   class_name_dict = {\"background\":0, \"apple\": 1, \"bean\": 2, \"boiled egg\": 3, \"chicken breast\": 4, \"fried egg\": 5, \"lunch\": 6, \n",
    "   \"rice\": 7, \"salad\": 8, \"spaguetti\": 9, \"steak\": 10}\n",
    "\n",
    "elif \"uecfoodpix\":\n",
    "   class_name_dict = {'background': 0, 'name': 1, 'rice': 2, 'eels on rice': 3, 'pilaf': 4, \"chicken-'n'-egg on rice\": 5, 'pork cutlet on rice': 6, \n",
    "   'beef curry': 7, 'sushi': 8, 'chicken rice': 9, 'fried rice': 10, 'tempura bowl': 11, 'bibimbap': 12, 'toast': 13, 'croissant': 14, 'roll bread': 15, \n",
    "   'raisin bread': 16, 'chip butty': 17, 'hamburger': 18, 'pizza': 19, 'sandwiches': 20, 'udon noodle': 21, 'tempura udon': 22, 'soba noodle': 23, \n",
    "   'ramen noodle': 24, 'beef noodle': 25, 'tensin noodle': 26, 'fried noodle': 27, 'spaghetti': 28, 'Japanese-style pancake': 29, 'takoyaki': 30, \n",
    "   'gratin': 31, 'sauteed vegetables': 32, 'croquette': 33, 'grilled eggplant': 34, 'sauteed spinach': 35, 'vegetable tempura': 36, 'miso soup': 37, \n",
    "   'potage': 38, 'sausage': 39, 'oden': 40, 'omelet': 41, 'ganmodoki': 42, 'jiaozi': 43, 'stew': 44, 'teriyaki grilled fish': 45, 'fried fish': 46, \n",
    "   'grilled salmon': 47, 'salmon meuniere': 48, 'sashimi': 49, 'grilled pacific saury': 50, 'sukiyaki': 51, 'sweet and sour pork': 52, \n",
    "   'lightly roasted fish': 53, 'steamed egg hotchpotch': 54, 'tempura': 55, 'fried chicken': 56, 'sirloin cutlet': 57, 'nanbanzuke': 58, \n",
    "   'boiled fish': 59, 'seasoned beef with potatoes': 60, 'hambarg steak': 61, 'beef steak': 62, 'dried fish': 63, 'ginger pork saute': 64, \n",
    "   'spicy chili-flavored tofu': 65, 'yakitori': 66, 'cabbage roll': 67, 'rolled omelet': 68, 'egg sunny-side up': 69, 'fermented soybeans': 70, \n",
    "   'cold tofu': 71, 'egg roll': 72, 'chilled noodle': 73, 'stir-fried beef and peppers': 74, 'simmered pork': 75, 'boiled chicken and vegetables': 76, \n",
    "   'sashimi bowl': 77, 'sushi bowl': 78, 'fish-shaped pancake with bean jam': 79, 'shrimp with chill source': 80, 'roast chicken': 81, \n",
    "   'steamed meat dumpling': 82, 'omelet with fried rice': 83, 'cutlet curry': 84, 'spaghetti meat sauce': 85, 'fried shrimp': 86, 'potato salad': 87, \n",
    "   'green salad': 88, 'macaroni salad': 89, 'Japanese tofu and vegetable chowder': 90, 'pork miso soup': 91, 'chinese soup': 92, 'beef bowl': 93, \n",
    "   'kinpira-style sauteed burdock': 94, 'rice ball': 95, 'pizza toast': 96, 'dipping noodles': 97, 'hot dog': 98, 'french fries': 99, 'mixed rice': 100, \n",
    "   'goya chanpuru': 101, 'others': 102, 'beverage': 103}\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "ROOT_DATASET_PATH = \"public/CamerFood10v2\"\n",
    "EPOCHS = 250\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 512\n",
    "LOGS_DIR = \"public/logs2/camerfood_res101\"\n",
    "NB_CLASS = len(class_name_dict)\n",
    "BACKBONE = \"res50\" # res101, res50, xception\n",
    "\n",
    "if not os.path.exists(LOGS_DIR):\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(LOGS_DIR)\n",
    "\n",
    "## Create the dataset\n",
    "DATASET_PATH = os.path.join(ROOT_DATASET_PATH, \"test\")\n",
    "test_dataset = CreateDataset(DATASET_PATH, DATASET_PATH, IMAGE_SIZE, BATCH_SIZE).get()\n",
    "\n",
    "DATASET_PATH = os.path.join(ROOT_DATASET_PATH, \"train\")\n",
    "train_dataset = CreateDataset(DATASET_NAME, DATASET_PATH, IMAGE_SIZE, BATCH_SIZE).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and Save the model architecture\n",
    "try:\n",
    "    with open(os.path.join(LOGS_DIR,\"model.json\"), \"x\") as json_file:\n",
    "        # model = DeepLabv3p(num_classes=NB_CLASS, encoder_name=\"res101\", input_shape=(512, 512, 3))()\n",
    "        model = DeepLabv3p_mid(num_classes=NB_CLASS, backbone_name=BACKBONE, finetune=True, input_shape=(512, 512, 3))()\n",
    "        # model = Unet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), classes=NB_CLASS)\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        json_file.write(model_json)\n",
    "except FileExistsError:\n",
    "        print(\"Oops!  This file already exist. Maybe the model have already been saved...\")\n",
    "        \n",
    "## Load the model architecture\n",
    "json_file = open(os.path.join(LOGS_DIR,\"model.json\"), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# At loading time, register the custom objects with a `custom_object_scope`:\n",
    "custom_objects = {\"SimAM\": SimAM}\n",
    "model = None\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "    # model.summary()\n",
    "\n",
    "class MeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self,\n",
    "               y_true=None,\n",
    "               y_pred=None,\n",
    "               num_classes=None,\n",
    "               name=None,\n",
    "               dtype=None):\n",
    "        super(MeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "epoch_start = 0\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.8, patience=30),\n",
    "    tf.keras.callbacks.CSVLogger(os.path.join(LOGS_DIR,\"data.csv\"), append=True, separator=\",\"),\n",
    "    # tf.keras.callbacks.TensorBoard(log_dir=LOGS_DIR, histogram_freq=0, write_graph=True, write_images=False),\n",
    "    tf.keras.callbacks.ModelCheckpoint(os.path.join(LOGS_DIR,\"saved_model_{epoch:02d}.h5\"), monitor='val_mIoU', verbose=0,\n",
    "                    save_best_only=True,  save_weights_only=False, mode='max', save_freq='epoch')\n",
    "]\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy', MeanIoU(num_classes=NB_CLASS, name='mIoU')])\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset, callbacks=callbacks, initial_epoch=epoch_start)\n",
    "model.save_weights(os.path.join(LOGS_DIR,\"model.h5\"), overwrite=True)\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
